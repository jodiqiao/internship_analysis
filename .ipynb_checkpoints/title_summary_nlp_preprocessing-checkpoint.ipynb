{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indeed.ca Internship Summary"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "git@github.com:jodiqiao/internship_analysis.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"sept_final_scrape.csv\",usecols = ['job_title','company_name','summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary text give insight to what qualifications are needed for what job?\n",
    "# qualifications different between companies even if job title similar?\n",
    "# company-specific preference of qualifications?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = df['summary'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "tokens = nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.digitalocean.com/community/tutorials/how-to-work-with-language-data-in-python-3-using-the-natural-language-toolkit-nltk\n",
    "# https://blog.algorithmia.com/introduction-natural-language-processing-nlp\n",
    "# https://www.kaggle.com/itratrahman/nlp-tutorial-using-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from matplotlib import pyplot as plt\n",
    "# import seaborn as sns\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/29523254/python-remove-stop-words-from-pandas-dataframe\n",
    "# compare nltk v sklearn stopwords\n",
    "# Import stopwords with nltk.\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "# stop = stopwords.words('english') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stopwords:  179\n"
     ]
    }
   ],
   "source": [
    "# np.array(stop)\n",
    "# print(\"Number of stopwords: \", len(stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "stop = text.ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stopwords:  318\n"
     ]
    }
   ],
   "source": [
    "np.array(stop)\n",
    "print(\"Number of stopwords: \", len(stop))\n",
    "\n",
    "# i like this one better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pythonhealthcare.org/2018/12/14/101-pre-processing-data-tokenization-stemming-and-removal-of-stop-words/\n",
    "# tokenization, stemming, and removal of stop words\n",
    "# i don't think i'm going to do sentiment analysis... but it's interesting\n",
    "# convert to lower case\n",
    "\n",
    "df['job_title'] = df['job_title'].str.lower()\n",
    "df['company_name'] = df['company_name'].str.lower()\n",
    "df['summary'] = df['summary'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize, remove punctuation, remove stopwords\n",
    "\n",
    "def identify_tokens(row):\n",
    "    #data_in_row = row['job_title']\n",
    "    #tokens = nltk.word_tokenize(data_in_row) # is a list of tokens\n",
    "    tokens = nltk.word_tokenize(row)\n",
    "    # taken only words (not punctuation)\n",
    "    no_punc_no_stop = []\n",
    "    for i in range(len(tokens)):\n",
    "        if \"/\" in tokens[i]:\n",
    "            # https://stackoverflow.com/questions/9797357/dividing-a-string-at-various-punctuation-marks-using-split\n",
    "            temp = tokens[i].replace('/',' ').replace('-',' ').split() # this split up web/front, coop/-intern; remove \"/\",\"-\" keep both wrds\n",
    "            for a in temp:\n",
    "                if tokens[i] not in stop:\n",
    "                    no_punc_no_stop.append(a)\n",
    "        if tokens[i].isalpha() and tokens[i] != \"r\" and tokens[i] != \"d\" and tokens[i] not in stop: # capture text that isn't 'r' or '&' or 'd'\n",
    "            no_punc_no_stop.append(tokens[i])\n",
    "        elif tokens[i].isalpha() and tokens[i] == \"r\": # idk what other words could become just an r\n",
    "            if tokens[i+1] == \"&\" and tokens[i+2] == \"d\": # r & d\n",
    "                no_punc_no_stop.append(\"r&d\")\n",
    "                i += 2\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "    \n",
    "    return no_punc_no_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = identify_tokens(df['job_title'][957])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['software', 'tester', 'sam']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = identify_tokens(df['summary'][957])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['software',\n",
       " 'tester',\n",
       " 'sam',\n",
       " 'summer',\n",
       " 'op',\n",
       " 'network',\n",
       " 'management',\n",
       " 'business',\n",
       " 'unit',\n",
       " 'nokia',\n",
       " 'ip',\n",
       " 'division',\n",
       " 'recruiting',\n",
       " 'highly',\n",
       " 'motivated',\n",
       " 'software',\n",
       " 'testers']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test stemming\n",
    "# from nltk.stem import PorterStemmer\n",
    "# stemming = PorterStemmer()\n",
    "# my_list = ['frightening', 'frightened', 'frightens']\n",
    "# Using a Python list comprehension method to apply to all words in my_list\n",
    "# print ([stemming.stem(word) for word in my_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print ([stemming.stem(word) for word in b])\n",
    "# output :\n",
    "# ['softwar', 'tester', 'sam', 'summer', 'op', 'network', 'manag', 'busi',\n",
    "# 'unit', 'nokia', 'ip', 'divis', 'recruit', 'highli', 'motiv', 'softwar', 'tester']\n",
    "# no stemming, this is not great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
